{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46854211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "class DataGathering: \n",
    "    \n",
    "    \"\"\" \n",
    "        Class to represent the pre-processing and data transformation steps. \n",
    "\n",
    "        Attributes: \n",
    "            file_path (str) : Name of the data file. \n",
    "            target (str) : Target of the data. \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, calibration_size = None, test_size=0.2, val_size = 0.3, batch_size = None, pipeline_num_list=None,  pipeline_cat_list=None, dict_o =None, target=None, data_type = None, pipeline_numerical=None, pipeline_categorical=None, pipeline_bin = None, bin_list = None, pca= None, tsne = None, umap = None, n_components = 5, autoencoder = None, separator = None):\n",
    "\n",
    "        \"\"\"\n",
    "            Initializes the pre-processing. \n",
    "    \n",
    "            Parameters: \n",
    "                file_path (str) : Name of the data file. \n",
    "                target (str) : Target of the data. \n",
    "                scaler (str) : Scaler for standardization or normalization of data. \n",
    "                imputer (str) : Imputer for missing value treatment. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        self.target = target\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy = \"median\")\n",
    "        self.pipeline_numerical = pipeline_numerical \n",
    "        self.pipeline_categorical = pipeline_categorical\n",
    "        self.pipeline_bin = pipeline_bin\n",
    "        self.pipeline_num_list = pipeline_num_list\n",
    "        self.pipeline_cat_list = pipeline_cat_list \n",
    "        self.pca = pca\n",
    "        self.tsne = tsne\n",
    "        self.umap = umap\n",
    "        self.n_components = n_components\n",
    "        self.autoencoder = autoencoder\n",
    "        self.dict_o = dict_o\n",
    "        self.separator = separator\n",
    "        self.bin_list = bin_list\n",
    "        self.calibration_size = calibration_size\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.data_type = data_type\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def get_preprocessing_step(self, method): \n",
    "\n",
    "        if method == \"imputer_num\": \n",
    "            return SimpleImputer(strategy = \"median\")\n",
    "\n",
    "        elif method == \"scaler\" :\n",
    "            return StandardScaler()\n",
    "\n",
    "        elif method == \"imputer_cat\": \n",
    "            return SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "        elif method == \"onehot\":\n",
    "            return OneHotEncoder()\n",
    "\n",
    "        else: \n",
    "            raise ValueError(\"Please enter a valid response !\")\n",
    "            \n",
    "            \n",
    "    def get_data_objects(self):\n",
    "        #https://medium.com/@paghadalsneh/encoding-numerical-data-with-discretization-and-binning-7fcea2659cbc\n",
    "        #https://pandas.pydata.org/docs/user_guide/text.html?utm_source=chatgpt.com\n",
    "        \"\"\"\n",
    "            Preprocessing step that is identifed by pipelines and column transformer. \n",
    "\n",
    "            Returns: \n",
    "                preprocessor (transformer): Preprocessor to transform the data into an appropriate format. \n",
    "        \"\"\"\n",
    "\n",
    "        df_ = pd.read_csv(self.file_path, encoding=\"utf-8-sig\", sep= self.separator, skipinitialspace=True)  \n",
    "\n",
    "\n",
    "        df = df_.drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        if self.target is not None:\n",
    "            \n",
    "            X = df.drop(columns=[self.target])\n",
    "            if self.dict_o is not None: \n",
    "                y = df[self.target].astype(str).str.strip().str.lower().map(self.dict_o).astype(\"int8\")\n",
    "            else: \n",
    "                y = df[self.target].astype(str).str.strip().str.lower().astype(\"int8\")\n",
    "                \n",
    "            bins = self.bin_list\n",
    "            numerical_columns = X.select_dtypes(include=['number']).columns.to_list()\n",
    "            categorical_columns = X.select_dtypes(include=['object']).columns.to_list()\n",
    "\n",
    "        else: \n",
    "            X = df\n",
    "            numerical_columns = X.select_dtypes(include=['number']).columns.to_list()\n",
    "            categorical_columns = X.select_dtypes(include=['object']).columns.to_list()\n",
    "            bins = self.bin_list\n",
    "\n",
    "        steps_num = []\n",
    "        steps_cat = []\n",
    "        if (self.pipeline_num_list is not None) or (self.pipeline_cat_list is not None):\n",
    "            for i in range(len(self.pipeline_num_list)):\n",
    "                steps_num.append((f\"{self.pipeline_num_list[i]}\", self.get_preprocessing_step(self.pipeline_num_list[i])))\n",
    "                self.pipeline_numerical = Pipeline(steps=steps_num)\n",
    "\n",
    "            for j in range((len(self.pipeline_cat_list))):\n",
    "                steps_cat.append((f\"{self.pipeline_cat_list[j]}\",self.get_preprocessing_step(self.pipeline_cat_list[j])))\n",
    "                self.pipeline_categorical = Pipeline(steps=steps_cat)\n",
    "                \n",
    "        if self.bin_list is not None:\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    ('pipeline_num',self.pipeline_numerical, numerical_columns),\n",
    "                    ('pipeline_bin', self.pipeline_bin, bins),\n",
    "                    ('pipeline_cat',self.pipeline_categorical, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        else: \n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                    ('pipeline_num',self.pipeline_numerical, numerical_columns),\n",
    "                    ('pipeline_cat',self.pipeline_categorical, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "     \n",
    "        print(preprocessor)\n",
    "        return preprocessor\n",
    "         \n",
    "\n",
    "        \n",
    "    def main_split(self):\n",
    "\n",
    "        \"\"\"\n",
    "            Applies the preprocessor and concatanates the X and y to create the training and test data.\n",
    "\n",
    "            Returns: \n",
    "                X_train_df (data frame): Data frame for training.\n",
    "                X_test_df (data frame): Data frame for testing.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_ = pd.read_csv(self.file_path, encoding=\"utf-8-sig\", sep= self.separator, skipinitialspace=True)  \n",
    "\n",
    "        df = df_.drop_duplicates().reset_index(drop=True)\n",
    "            \n",
    "        if self.target is not None:\n",
    "                \n",
    "            X = df.drop(columns=[self.target])\n",
    "            if self.dict_o is not None: \n",
    "                y = df[self.target].astype(str).str.strip().str.lower().map(self.dict_o).astype(\"int8\")\n",
    "            else: \n",
    "                y = df[self.target].astype(str).str.strip().str.lower().astype(\"int8\")\n",
    "    \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = self.test_size)\n",
    "    \n",
    "            X_train_df = X_train.reset_index(drop = True)\n",
    "            X_test_df = X_test.reset_index(drop = True)\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "                \n",
    "            return X_train_df, y_train, X_test_df, y_test\n",
    "        \n",
    "        else: \n",
    "            X = df\n",
    "            X_train, X_test = train_test_split(X, test_size = self.test_size)\n",
    "            \n",
    "            X_train_df = X_train.reset_index(drop = True)\n",
    "            X_test_df = X_test.reset_index(drop = True)\n",
    "          \n",
    "\n",
    "            return X_train_df, X_test_df\n",
    "\n",
    "\n",
    "    def calibration_loading(self, X_train, y_train):\n",
    "        \n",
    "        if self.calibration_size is not None:\n",
    "                \n",
    "            X_train_, X_calib, y_train_, y_calib = train_test_split(X_train, y_train, stratify = y_train, test_size = self.calibration_size)\n",
    "            \n",
    "            \n",
    "            return X_train_, y_train_, X_calib, y_calib\n",
    "\n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "    def data_loading(self, imbalance_handler = \"none\"):\n",
    "\n",
    "        \"\"\"\n",
    "            Prepares a data loader for neural networks. \n",
    "\n",
    "            Returns: \n",
    "                data_loader_train (data loader): Data loader for training.\n",
    "                data_loader_test (data loader): Data loader for testing. \n",
    "        \"\"\"\n",
    "        \n",
    "        df_ = pd.read_csv(self.file_path, encoding=\"utf-8-sig\", sep= self.separator, skipinitialspace=True)  \n",
    "\n",
    "        df = df_.drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        X = df.drop(columns=[self.target])\n",
    "        if self.dict_o is not None: \n",
    "                y = df[self.target].astype(str).str.strip().str.lower().map(self.dict_o).astype(\"int8\")\n",
    "        else: \n",
    "            y = df[self.target].astype(str).str.strip().str.lower().astype(\"int8\")\n",
    "\n",
    "        X_train_a, X_valid_a, y_train, y_valid_a = train_test_split(X, y, stratify = y, test_size = self.test_size)\n",
    "        X_valid_b, X_test_b, y_valid, y_test = train_test_split(X_valid_a, y_valid_a, test_size = self.val_size)\n",
    "        \n",
    "        preprocessor = self.get_data_objects()\n",
    "        X_train_p = preprocessor.fit_transform(X_train_a)\n",
    "        X_valid_p = preprocessor.transform(X_valid_b)\n",
    "        X_test_p = preprocessor.transform(X_test_b)\n",
    "\n",
    "        if imbalance_handler.method == \"classweights\": \n",
    "\n",
    "            X_res = X_train_p\n",
    "            y_res = y_train\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            X_res, y_res = imbalance_handler.resampling(X_train_p, y_train)\n",
    "        \n",
    "        columns = preprocessor.get_feature_names_out()\n",
    "        X_train = pd.DataFrame(X_res, columns = columns).reset_index(drop=True)\n",
    "        X_valid = pd.DataFrame(X_valid_p, columns = columns).reset_index(drop=True)\n",
    "        X_test = pd.DataFrame(X_test_p, columns = columns).reset_index(drop=True)\n",
    "        y_res = y_res.reset_index(drop=True)\n",
    "        \n",
    "        if self.autoencoder == \"yes\":\n",
    "            \n",
    "            X_train_ae = X_train[y_res==0]\n",
    "            y_train_ae = y_res[y_res==0]\n",
    "\n",
    "    \n",
    "            X_tensor = torch.tensor(X_train_ae.values, dtype = torch.float32)\n",
    "            y_tensor = torch.tensor(y_train_ae.values, dtype = self.data_type)\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            X_tensor = torch.tensor(X_train.values, dtype = torch.float32)\n",
    "            y_tensor = torch.tensor(y_res.values, dtype = self.data_type)\n",
    "\n",
    "    \n",
    "        X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "        y_valid_tensor = torch.tensor(y_valid.values, dtype=self.data_type)\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype = torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype = self.data_type)\n",
    "        \n",
    "\n",
    "        tensor_data = TensorDataset(X_tensor, y_tensor)\n",
    "        tensor_valid_data = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "        tensor_test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        \n",
    "        data_loader_train = DataLoader(tensor_data, batch_size = self.batch_size, shuffle = True)\n",
    "        data_loader_valid = DataLoader(tensor_valid_data, batch_size=self.batch_size)\n",
    "        data_loader_test = DataLoader(tensor_test_data, batch_size = self.batch_size)\n",
    "        \n",
    "        return data_loader_train, data_loader_valid, data_loader_test \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
